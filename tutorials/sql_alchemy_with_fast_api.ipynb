{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAPI App with Async SQLALchemy 2.0 & Pydantic V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early this year, a major update was made to SQLAlchemy with the release of SQLAlchemy 2.0. Among other things, this includes significant updates to basic ORM syntax and to some technical machinery necessary for a good experience using an async engine with asyncio. (You can read more about these updates here: https://docs.sqlalchemy.org/en/20/changelog/whatsnew_20.html.)\n",
    "\n",
    "Improvements to the async experience are a boon for the use of SQLAlchemy with FastAPI in particular, as having an async database sesson means that you no longer have to worry about database operations blocking async path operations. In short, it offers a big speed-up over a synchronous database setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am accustomed to using Docker for local development, so I started this project using Docker Compose."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "version: \"3.7\"\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:15-alpine\n",
    "    restart: always\n",
    "    environment:\n",
    "      POSTGRES_USER: dev-user\n",
    "      POSTGRES_PASSWORD: password\n",
    "      POSTGRES_DB: dev_db\n",
    "    ports:\n",
    "      - 5432:5432\n",
    "    expose:\n",
    "      - \"5432\"\n",
    "    volumes:\n",
    "      - db-data:/var/lib/postgresql/data:cached\n",
    "\n",
    "  test-postgres:\n",
    "    image: postgres:15-alpine\n",
    "    restart: always\n",
    "    environment:\n",
    "      POSTGRES_USER: test-user\n",
    "      POSTGRES_PASSWORD: password\n",
    "      POSTGRES_DB: test_db\n",
    "    ports:\n",
    "      - 5434:5432 # Use a different port to avoid conflicts with the main database\n",
    "    expose:\n",
    "      - \"5434\" # Exposing the different port for clarity\n",
    "    volumes:\n",
    "      - test-db-data:/var/lib/postgresql/data:cached\n",
    "\n",
    "  backend:\n",
    "    build:\n",
    "      context: backend\n",
    "      dockerfile: Dockerfile\n",
    "    command: python app/main.py\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ./backend:/backend/:cached\n",
    "      - ./.docker/.ipython:/root/.ipython:cached\n",
    "    environment:\n",
    "      PYTHONPATH: .\n",
    "      DATABASE_URL: \"postgresql+asyncpg://dev-user:password@postgres:5432/dev_db\"\n",
    "    depends_on:\n",
    "      - \"postgres\"\n",
    "    ports:\n",
    "      - 8000:8000\n",
    "\n",
    "  frontend:\n",
    "    build:\n",
    "      context: frontend\n",
    "      dockerfile: Dockerfile\n",
    "    stdin_open: true\n",
    "    volumes:\n",
    "      - \"./frontend:/app:cached\"\n",
    "      - \"./frontend/node_modules:/app/node_modules:cached\"\n",
    "    environment:\n",
    "      - NODE_ENV=development\n",
    "    ports:\n",
    "      - 3000:3000\n",
    "\n",
    "volumes:\n",
    "  db-data:\n",
    "  test-db-data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main things you should see are that I am using Postgres for my DB and asyncpg as my database interface library, and that I have set up two persistent DBs for local development: one for running the app and another for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, here is the Dockerfile I’m using for the backend container:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "FROM python:3.11.4\n",
    "\n",
    "RUN mkdir /backend\n",
    "WORKDIR /backend\n",
    "\n",
    "RUN apt update && \\\n",
    "    apt install -y postgresql-client\n",
    "\n",
    "COPY requirements.txt ./\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why dockerize your local DB instances?\n",
    "This makes it easy for new developers to spin up this project without having to install a bunch of complicated libraries.\n",
    "\n",
    "Why use a persistent DB for testing?\n",
    "As we’ll cover later, the setup I’m using for Pytest ensures that no actual data persists to this DB from tests, but making the volume persistent does save us from running every single migration every time we want to run tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and Schemas\n",
    "In FastAPI the convention is to separate out your SQLAlchemy model classes from your Pydantic schemas — the SQLAlchemy classes are used only for defining the DB schema, the schemas are for validating incoming and outgoing data in your crud functions and path operations. (Pydantic schemas are sort of half-way between API serializer classes and dataclasses — which is a very handy hybrid to have.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it’s true that the creator of FastAPI has created a project, SQLModel, that tries to unify SQLAlchemy models and Pydantic schemas into one entity. But this library seems to be a little behind the curve in terms of keeping up with the latest developments in either SQLAlchemy or Pydantic. In any case, I prefer the pattern of separating the DB classes from the schemas. One reason to want this is to have different schemas for different CRUD operations — e.g. your schema for updates may have more optional types than your schema for reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with that in mind, how do we define our DB classes?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "models/\n",
    "    __init__.py\n",
    "    model_1.py\n",
    "    model_2.py\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, I tried to do the same thing using the latest way of defining model classes in SQLAlchemy 2.0, i.e.Mapped Classes. I was able to get this to work, but there’s a trick. If you want to expose your models in the __init__.py of this directory, you will need to define the required base class before you load in the models themselves. In other words, you will need to do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app/models/__init__.py\n",
    "\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "Base = declarative_base() # model base class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here you can define your first model class using the new SQLAlchemy pattern. Here’s the example from the demo project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Mapped, mapped_column\n",
    "\n",
    "from . import Base\n",
    "\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"user\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True, index=True)\n",
    "    username: Mapped[str] = mapped_column(index=True, unique=True)\n",
    "    slug: Mapped[str] = mapped_column(index=True, unique=True)\n",
    "    email: Mapped[str] = mapped_column(index=True, unique=True)\n",
    "    first_name: Mapped[str]\n",
    "    last_name: Mapped[str]\n",
    "    hashed_password: Mapped[str]\n",
    "    is_superuser: Mapped[bool] = mapped_column(default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto Pydantic schemas.\n",
    "\n",
    "Naturally enough, we can define the schemas for this table in /app/schemas/user.py.\n",
    "\n",
    "Here are some basic schemas that could be used for simple READ operations on the above User model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    model_config = ConfigDict(from_attributes=True)\n",
    "\n",
    "    id: int\n",
    "    username: str\n",
    "    slug: str\n",
    "    email: str\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    is_superuser: bool = False\n",
    "\n",
    "class UserPrivate(User):\n",
    "    hashed_password: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alembic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alembic is a database migration tool for SQLAlchemy. It allows you to:\n",
    "\n",
    "1. Track changes to your database schema over time\n",
    "2. Create migration scripts to update your database schema\n",
    "3. Apply or revert these migrations as needed\n",
    "\n",
    "Alembic is particularly useful in FastAPI projects with SQLAlchemy, as it helps manage database schema changes in a version-controlled, reproducible manner. This is essential for maintaining database consistency across different environments (development, staging, production) and when working in teams.     \n",
    "\n",
    "Alembic set up is alembic/env.py. Here’s the one from the demo project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from logging.config import fileConfig\n",
    "\n",
    "from alembic import context\n",
    "from asyncpg import Connection\n",
    "from sqlalchemy import pool\n",
    "from sqlalchemy.ext.asyncio import async_engine_from_config\n",
    "\n",
    "from app.models import Base\n",
    "\n",
    "# this is the Alembic Config object, which provides\n",
    "# access to the values within the .ini file in use.\n",
    "config = context.config\n",
    "\n",
    "# Interpret the config file for Python logging.\n",
    "# This line sets up loggers basically.\n",
    "fileConfig(config.config_file_name)  # type: ignore\n",
    "\n",
    "# add your model's MetaData object here\n",
    "target_metadata = Base.metadata\n",
    "\n",
    "\n",
    "def get_url():\n",
    "    return os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "\n",
    "def run_migrations_offline():\n",
    "    \"\"\"Run migrations in 'offline' mode.\n",
    "    This configures the context with just a URL\n",
    "    and not an Engine, though an Engine is acceptable\n",
    "    here as well.  By skipping the Engine creation\n",
    "    we don't even need a DBAPI to be available.\n",
    "    Calls to context.execute() here emit the given string to the\n",
    "    script output.\n",
    "    \"\"\"\n",
    "    # url = config.get_main_option(\"sqlalchemy.url\")\n",
    "    url = get_url()\n",
    "    context.configure(\n",
    "        url=url,\n",
    "        target_metadata=target_metadata,\n",
    "        literal_binds=True,\n",
    "        dialect_opts={\"paramstyle\": \"named\"},\n",
    "    )\n",
    "\n",
    "    with context.begin_transaction():\n",
    "        context.run_migrations()\n",
    "\n",
    "\n",
    "def do_run_migrations(connection: Connection) -> None:\n",
    "    context.configure(connection=connection, target_metadata=target_metadata)\n",
    "\n",
    "    with context.begin_transaction():\n",
    "        context.run_migrations()\n",
    "\n",
    "\n",
    "async def run_migrations_online():\n",
    "    \"\"\"Run migrations in 'online' mode.\n",
    "    In this scenario we need to create an Engine\n",
    "    and associate a connection with the context.\n",
    "    \"\"\"\n",
    "    configuration = config.get_section(config.config_ini_section)\n",
    "    configuration[\"sqlalchemy.url\"] = get_url()\n",
    "    connectable = async_engine_from_config(\n",
    "        configuration,\n",
    "        prefix=\"sqlalchemy.\",\n",
    "        poolclass=pool.NullPool,\n",
    "    )\n",
    "\n",
    "    async with connectable.connect() as connection:\n",
    "        await connection.run_sync(do_run_migrations)\n",
    "\n",
    "    await connectable.dispose()\n",
    "\n",
    "\n",
    "if context.is_offline_mode():\n",
    "    run_migrations_offline()\n",
    "else:\n",
    "    asyncio.run(run_migrations_online())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here you can autogenerate your first migration file like so:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose exec backend alembic revision --autogenerate -m \"Initial revision\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a new migration file in the migrations directory. You can then apply the migration to your database using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose exec backend alembic upgrade head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will apply all pending migrations to your database, updating the schema to match the latest version defined in your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s a handy spinoff of Pydantic called pydantic-settings that provides a typesafe settings class that automatically loads values from env variables. Here’s a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    database_url: str\n",
    "    echo_sql: bool = True\n",
    "    test: bool = False\n",
    "    project_name: str = \"My FastAPI project\"\n",
    "    oauth_token_secret: str = \"my_dev_secret\"\n",
    "\n",
    "\n",
    "settings = Settings()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an example of how this works: if you set an environment variable DATABASE_URL = “blah” then you will be able to instantly access that value in your app by importing this class instance. This is very handy for managing values in different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the really interesting part, how to set up your async engine and async database sessions. The ideal for this is that you can use the same set up in your app and in your tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from collections.abc import AsyncIterator\n",
    "from typing import Any\n",
    "\n",
    "from sqlalchemy.ext.asyncio import (\n",
    "    AsyncConnection,\n",
    "    AsyncSession,\n",
    "    async_sessionmaker,\n",
    "    create_async_engine,\n",
    ")\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "from app.config import settings\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Heavily inspired by https://praciano.com.br/fastapi-and-async-sqlalchemy-20-with-pytest-done-right.html\n",
    "\n",
    "\n",
    "class DatabaseSessionManager:\n",
    "    def __init__(self, host: str, engine_kwargs: dict[str, Any] = {}):\n",
    "        self._engine = create_async_engine(host, **engine_kwargs)\n",
    "        self._sessionmaker = async_sessionmaker(autocommit=False, bind=self._engine)\n",
    "\n",
    "    async def close(self):\n",
    "        if self._engine is None:\n",
    "            raise Exception(\"DatabaseSessionManager is not initialized\")\n",
    "        await self._engine.dispose()\n",
    "\n",
    "        self._engine = None\n",
    "        self._sessionmaker = None\n",
    "\n",
    "    @contextlib.asynccontextmanager\n",
    "    async def connect(self) -> AsyncIterator[AsyncConnection]:\n",
    "        if self._engine is None:\n",
    "            raise Exception(\"DatabaseSessionManager is not initialized\")\n",
    "\n",
    "        async with self._engine.begin() as connection:\n",
    "            try:\n",
    "                yield connection\n",
    "            except Exception:\n",
    "                await connection.rollback()\n",
    "                raise\n",
    "\n",
    "    @contextlib.asynccontextmanager\n",
    "    async def session(self) -> AsyncIterator[AsyncSession]:\n",
    "        if self._sessionmaker is None:\n",
    "            raise Exception(\"DatabaseSessionManager is not initialized\")\n",
    "\n",
    "        session = self._sessionmaker()\n",
    "        try:\n",
    "            yield session\n",
    "        except Exception:\n",
    "            await session.rollback()\n",
    "            raise\n",
    "        finally:\n",
    "            await session.close()\n",
    "\n",
    "\n",
    "sessionmanager = DatabaseSessionManager(settings.database_url, {\"echo\": settings.echo_sql})\n",
    "\n",
    "\n",
    "async def get_db_session():\n",
    "    async with sessionmanager.session() as session:\n",
    "        yield session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I may as well finally show you the main file. This looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "\n",
    "from app.api.routers.users import router as users_router\n",
    "from app.config import settings\n",
    "from app.database import sessionmanager\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG if settings.debug_logs else logging.INFO)\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"\n",
    "    Function that handles startup and shutdown events.\n",
    "    To understand more, read https://fastapi.tiangolo.com/advanced/events/\n",
    "    \"\"\"\n",
    "    yield\n",
    "    if sessionmanager._engine is not None:\n",
    "        # Close the DB connection\n",
    "        await sessionmanager.close()\n",
    "\n",
    "\n",
    "app = FastAPI(lifespan=lifespan, title=settings.project_name, docs_url=\"/api/docs\")\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Hello World\"}\n",
    "\n",
    "\n",
    "# Routers\n",
    "app.include_router(users_router)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", reload=True, port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the lifespan context manager which will close the DB connection automatically when the app is closed.\n",
    "\n",
    "Now, recall the get_db_session function from the database.py file shared above. This can become our fundamental db interface across the app. First, we turn it into a dependency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from fastapi import Depends\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "\n",
    "from app.database import get_db_session\n",
    "\n",
    "DBSessionDep = Annotated[AsyncSession, Depends(get_db_session)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import APIRouter, Depends\n",
    "\n",
    "from app.api.dependencies.auth import validate_is_authenticated\n",
    "from app.api.dependencies.core import DBSessionDep\n",
    "from app.crud.user import get_user\n",
    "from app.schemas.user import User\n",
    "\n",
    "router = APIRouter(\n",
    "    prefix=\"/api/users\",\n",
    "    tags=[\"users\"],\n",
    "    responses={404: {\"description\": \"Not found\"}},\n",
    ")\n",
    "\n",
    "@router.get(\n",
    "    \"/{user_id}\",\n",
    "    response_model=User,\n",
    "    dependencies=[Depends(validate_is_authenticated)],\n",
    ")\n",
    "async def user_details(\n",
    "    user_id: int,\n",
    "    db_session: DBSessionDep,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get any user details\n",
    "    \"\"\"\n",
    "    user = await get_user(db_session, user_id)\n",
    "    return user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRUD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a database session in hand, we can begin to define CRUD functions like the get_user function defined in the previous snippet. Using the latest SQLAlchemy syntax, the implementation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import HTTPException\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "\n",
    "from app.models import User as UserDBModel\n",
    "\n",
    "\n",
    "async def get_user(db_session: AsyncSession, user_id: int) -> UserDBModel:\n",
    "    user = (await db_session.scalars(select(UserDBModel).where(UserDBModel.id == user_id))).first()\n",
    "    if not user:\n",
    "        raise HTTPException(status_code=404, detail=\"User not found\")\n",
    "    return user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you are writing a Create or Update function, I would recommend not calling await db_session.commit() inside the function itself — instead do it in the path operation function. This is for two reasons:\n",
    "\n",
    "You will sometimes want to compose multiple CRUD functions and usually you will want them to occur in the same session (transaction).\n",
    "It makes test cleanup way easier (next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the code of my project, I'm using FastCrud for CRUD operations. It's a library that provides a set of decorators for FastAPI that make it easier to work with SQLAlchemy models. It's a very handy library and I highly recommend it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Testing with Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytest is an insanely powerful and versatile Python testing framework for unittests and integration tests. The main configuration file that Pytest uses is called conftest.py. In this file, you can define the “fixtures” (basically, pre-computed functions) that you need for your tests. These fixtures are highly controllable: they can be run before every test, or at the start of the entire test session; they can be run in the background, or you can use the return value as an argument to your test functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requirements for my conftest.py were as follows:\n",
    "\n",
    "At the start of the session, it should automatically connect to the test DB and run migrations if any are missing.\n",
    "Each test function should be a clean state — nothing should be written to the test DB.\n",
    "We should be able to use db_session as a test argument or have it set up automatically in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from contextlib import ExitStack\n",
    "\n",
    "import pytest\n",
    "from alembic.config import Config\n",
    "from alembic.migration import MigrationContext\n",
    "from alembic.operations import Operations\n",
    "from alembic.script import ScriptDirectory\n",
    "from asyncpg import Connection\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "from app.config import settings\n",
    "from app.database import Base, get_db_session\n",
    "from app.main import app as actual_app\n",
    "\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def app():\n",
    "    with ExitStack():\n",
    "        yield actual_app\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def client(app):\n",
    "    with TestClient(app) as c:\n",
    "        yield c\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def event_loop(request):\n",
    "    loop = asyncio.get_event_loop_policy().new_event_loop()\n",
    "    yield loop\n",
    "    loop.close()\n",
    "\n",
    "\n",
    "def run_migrations(connection: Connection):\n",
    "    config = Config(\"app/alembic.ini\")\n",
    "    config.set_main_option(\"script_location\", \"app/alembic\")\n",
    "    config.set_main_option(\"sqlalchemy.url\", settings.database_url)\n",
    "    script = ScriptDirectory.from_config(config)\n",
    "\n",
    "    def upgrade(rev, context):\n",
    "        return script._upgrade_revs(\"head\", rev)\n",
    "\n",
    "    context = MigrationContext.configure(connection, opts={\"target_metadata\": Base.metadata, \"fn\": upgrade})\n",
    "\n",
    "    with context.begin_transaction():\n",
    "        with Operations.context(context):\n",
    "            context.run_migrations()\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\", autouse=True)\n",
    "async def setup_database():\n",
    "    # Run alembic migrations on test DB\n",
    "    async with sessionmanager.connect() as connection:\n",
    "        await connection.run_sync(run_migrations)\n",
    "\n",
    "    yield\n",
    "\n",
    "    # Teardown\n",
    "    await sessionmanager.close()\n",
    "\n",
    "\n",
    "# Each test function is a clean slate\n",
    "@pytest.fixture(scope=\"function\", autouse=True)\n",
    "async def transactional_session():\n",
    "    async with sessionmanager.session() as session:\n",
    "        try:\n",
    "            await session.begin()\n",
    "            yield session\n",
    "        finally:\n",
    "            await session.rollback()  # Rolls back the outer transaction\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\")\n",
    "async def db_session(transactional_session):\n",
    "    yield transactional_session\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=True)\n",
    "async def session_override(app, db_session):\n",
    "    async def get_db_session_override():\n",
    "        yield db_session[0]\n",
    "\n",
    "    app.dependency_overrides[get_db_session] = get_db_session_override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only detail missing here is we need to set settings.database_url to be the test database. The way I am doing this is when executing the test command itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "TEST=1 DATABASE_URL=\"postgresql+asyncpg://test-user:password@test-postgres:5432/test_db\" docker compose run --rm -e TEST -e DATABASE_URL backend pytest \"path/to/tests\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this confest.py in place, define your pytest.ini to handle async test functions as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[pytest]\n",
    "asyncio_mode = auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you want to avoid calling await db_session.commit() in these test functions because that will write to the test DB. If you must, you will need to follow the latest “savepoint” (nested transaction) docs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
