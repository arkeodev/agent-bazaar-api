{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating RabbitMQ and Kafka with FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll explore how to integrate **RabbitMQ** and **Apache Kafka** with **FastAPI** to build scalable, robust, and asynchronous applications. We'll cover the fundamental concepts of message brokers, how to set up RabbitMQ and Kafka, and demonstrate how to use them within a FastAPI application. By the end of this tutorial, you'll be able to leverage these message brokers to enhance the performance and reliability of your FastAPI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Prerequisites](#2-prerequisites)\n",
    "3. [Understanding Message Brokers](#3-understanding-message-brokers)\n",
    "   - [3.1. What is RabbitMQ?](#31-what-is-rabbitmq)\n",
    "   - [3.2. What is Apache Kafka?](#32-what-is-apache-kafka)\n",
    "   - [3.3. RabbitMQ vs. Kafka](#33-rabbitmq-vs-kafka)\n",
    "4. [Setting Up the Environment](#4-setting-up-the-environment)\n",
    "5. [Integrating RabbitMQ with FastAPI](#5-integrating-rabbitmq-with-fastapi)\n",
    "   - [5.1. Installing Dependencies](#51-installing-dependencies)\n",
    "   - [5.2. Setting Up RabbitMQ](#52-setting-up-rabbitmq)\n",
    "   - [5.3. Publishing Messages](#53-publishing-messages)\n",
    "   - [5.4. Consuming Messages](#54-consuming-messages)\n",
    "6. [Integrating Kafka with FastAPI](#6-integrating-kafka-with-fastapi)\n",
    "   - [6.1. Installing Dependencies](#61-installing-dependencies)\n",
    "   - [6.2. Setting Up Kafka](#62-setting-up-kafka)\n",
    "   - [6.3. Producing Messages](#63-producing-messages)\n",
    "   - [6.4. Consuming Messages](#64-consuming-messages)\n",
    "7. [Comparing RabbitMQ and Kafka Integration](#7-comparing-rabbitmq-and-kafka-integration)\n",
    "8. [Implementing Asynchronous Tasks](#8-implementing-asynchronous-tasks)\n",
    "9. [Error Handling and Retries](#9-error-handling-and-retries)\n",
    "10. [Testing the Application](#10-testing-the-application)\n",
    "11. [Conclusion](#11-conclusion)\n",
    "12. [References](#12-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Message brokers like **RabbitMQ** and **Apache Kafka** play a crucial role in building distributed systems by enabling asynchronous communication between services. They help in decoupling services, improving scalability, and handling high-throughput data streams.\n",
    "\n",
    "In this tutorial, we'll:\n",
    "\n",
    "- Understand what RabbitMQ and Kafka are and their use cases.\n",
    "- Set up RabbitMQ and Kafka in your development environment.\n",
    "- Integrate RabbitMQ and Kafka with a FastAPI application.\n",
    "- Implement message publishing and consuming.\n",
    "- Compare the integration processes and use cases for RabbitMQ and Kafka.\n",
    "- Handle errors and test the application effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites\n",
    "\n",
    "Before we begin, ensure you have the following:\n",
    "\n",
    "- **Python 3.7+** installed.\n",
    "- Basic knowledge of **Python** and **FastAPI**.\n",
    "- Familiarity with concepts like **asynchronous programming** and **message queues**.\n",
    "- **Docker** installed to run RabbitMQ and Kafka in containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Message Brokers\n",
    "\n",
    "### 3.1. What is RabbitMQ?\n",
    "\n",
    "**RabbitMQ** is an open-source message broker that implements the Advanced Message Queuing Protocol (AMQP). It supports multiple messaging protocols and can be used for asynchronous communication between microservices, handling background tasks, and more.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Reliability**: Persistent messages, acknowledgments, and publisher confirms.\n",
    "- **Flexible Routing**: Exchanges can route messages based on patterns.\n",
    "- **Clustering and Federation**: For high availability and scalability.\n",
    "\n",
    "### 3.2. What is Apache Kafka?\n",
    "\n",
    "**Apache Kafka** is a distributed streaming platform used for building real-time data pipelines and streaming applications. It's designed for high-throughput, low-latency handling of real-time data feeds.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Scalability**: Handles high volumes of data with horizontal scalability.\n",
    "- **Durability**: Data is persisted on disk and replicated within the cluster.\n",
    "- **Stream Processing**: Supports real-time stream processing.\n",
    "\n",
    "### 3.3. RabbitMQ vs. Kafka\n",
    "\n",
    "| Feature            | RabbitMQ                                     | Apache Kafka                                          |\n",
    "|--------------------|----------------------------------------------|-------------------------------------------------------|\n",
    "| Use Case           | Message queueing, short-lived messages       | Event streaming, long-lived data                      |\n",
    "| Message Model      | Queue-based, consumers pull messages         | Log-based, consumers read from offsets                |\n",
    "| Delivery Semantics | At-most-once, at-least-once, exactly-once    | At-least-once (exactly-once with extra configurations)|\n",
    "| Ordering           | Per-queue ordering                           | Partition-level ordering                              |\n",
    "| Scalability        | Good for moderate loads                      | Designed for high throughput and scalability          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting Up the Environment\n",
    "\n",
    "Create a new project directory and set up a virtual environment.\n",
    "\n",
    "```bash\n",
    "# Create project directory\n",
    "mkdir fastapi-messaging-tutorial\n",
    "cd fastapi-messaging-tutorial\n",
    "\n",
    "# Set up virtual environment\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Integrating RabbitMQ with FastAPI\n",
    "\n",
    "### 5.1. Installing Dependencies\n",
    "\n",
    "Install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install fastapi uvicorn[standard] aio-pika\n",
    "```\n",
    "\n",
    "- **fastapi**: The web framework.\n",
    "- **uvicorn**: ASGI server.\n",
    "- **aio-pika**: Asynchronous Python client for RabbitMQ.\n",
    "\n",
    "### 5.2. Setting Up RabbitMQ\n",
    "\n",
    "We'll use Docker to run RabbitMQ:\n",
    "\n",
    "```bash\n",
    "docker run -d --hostname rabbitmq --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3-management\n",
    "```\n",
    "\n",
    "- **Ports**:\n",
    "  - **5672**: AMQP port.\n",
    "  - **15672**: Management UI port.\n",
    "\n",
    "Access the RabbitMQ Management UI at `http://localhost:15672` (default username and password are `guest`).\n",
    "\n",
    "### 5.3. Publishing Messages\n",
    "\n",
    "**main.py**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "import aio_pika\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "async def get_rabbitmq_connection():\n",
    "    return await aio_pika.connect_robust(\"amqp://guest:guest@localhost/\")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    app.state.rabbitmq_connection = await get_rabbitmq_connection()\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    await app.state.rabbitmq_connection.close()\n",
    "\n",
    "async def publish_message(message: str):\n",
    "    connection = app.state.rabbitmq_connection\n",
    "    channel = await connection.channel()\n",
    "    queue = await channel.declare_queue(\"task_queue\", durable=True)\n",
    "    await channel.default_exchange.publish(\n",
    "        aio_pika.Message(\n",
    "            body=message.encode(),\n",
    "            delivery_mode=aio_pika.DeliveryMode.PERSISTENT,\n",
    "        ),\n",
    "        routing_key=queue.name,\n",
    "    )\n",
    "    await channel.close()\n",
    "\n",
    "@app.post(\"/tasks\")\n",
    "async def create_task(message: str, background_tasks: BackgroundTasks):\n",
    "    background_tasks.add_task(publish_message, message)\n",
    "    return {\"message\": \"Task received\"}\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- **get_rabbitmq_connection**: Establishes a connection to RabbitMQ.\n",
    "- **publish_message**: Publishes a message to the `task_queue`.\n",
    "- **BackgroundTasks**: Used to run `publish_message` in the background.\n",
    "- **Delivery Mode**: Set to `PERSISTENT` to ensure messages are saved to disk.\n",
    "\n",
    "### 5.4. Consuming Messages\n",
    "\n",
    "**worker.py**\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "import aio_pika\n",
    "\n",
    "async def main():\n",
    "    connection = await aio_pika.connect_robust(\"amqp://guest:guest@localhost/\")\n",
    "    queue_name = \"task_queue\"\n",
    "\n",
    "    async with connection:\n",
    "        channel = await connection.channel()\n",
    "        queue = await channel.declare_queue(queue_name, durable=True)\n",
    "\n",
    "        async with queue.iterator() as queue_iter:\n",
    "            async for message in queue_iter:\n",
    "                async with message.process():\n",
    "                    print(f\"Received message: {message.body.decode()}\")\n",
    "                    await asyncio.sleep(1)  # Simulate work\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- **worker.py**: A worker script that consumes messages from `task_queue`.\n",
    "- **message.process()**: Ensures messages are acknowledged after processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integrating Kafka with FastAPI\n",
    "\n",
    "### 6.1. Installing Dependencies\n",
    "\n",
    "Install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install fastapi uvicorn[standard] aiokafka\n",
    "```\n",
    "\n",
    "- **aiokafka**: Asynchronous Python client for Apache Kafka.\n",
    "\n",
    "### 6.2. Setting Up Kafka\n",
    "\n",
    "We'll use Docker Compose to set up Kafka along with ZooKeeper.\n",
    "\n",
    "**docker-compose.yml**\n",
    "\n",
    "```yaml\n",
    "version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: 'confluentinc/cp-zookeeper:latest'\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "\n",
    "  kafka:\n",
    "    image: 'confluentinc/cp-kafka:latest'\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "```\n",
    "\n",
    "Start Kafka and ZooKeeper:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "### 6.3. Producing Messages\n",
    "\n",
    "**main.py**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "from aiokafka import AIOKafkaProducer\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "loop = asyncio.get_event_loop()\n",
    "producer = AIOKafkaProducer(loop=loop, bootstrap_servers='localhost:9092')\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    await producer.start()\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    await producer.stop()\n",
    "\n",
    "async def send_message(topic: str, message: str):\n",
    "    await producer.send_and_wait(topic, message.encode())\n",
    "\n",
    "@app.post(\"/tasks\")\n",
    "async def create_task(message: str, background_tasks: BackgroundTasks):\n",
    "    background_tasks.add_task(send_message, \"task_topic\", message)\n",
    "    return {\"message\": \"Task received\"}\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- **AIOKafkaProducer**: Initializes a Kafka producer.\n",
    "- **send_message**: Sends a message to the specified Kafka topic.\n",
    "- **BackgroundTasks**: Used to run `send_message` in the background.\n",
    "\n",
    "### 6.4. Consuming Messages\n",
    "\n",
    "**consumer.py**\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from aiokafka import AIOKafkaConsumer\n",
    "\n",
    "async def consume():\n",
    "    consumer = AIOKafkaConsumer(\n",
    "        'task_topic',\n",
    "        bootstrap_servers='localhost:9092',\n",
    "        group_id=\"my-group\",\n",
    "    )\n",
    "    await consumer.start()\n",
    "    try:\n",
    "        async for msg in consumer:\n",
    "            print(f\"Received message: {msg.value.decode()}\")\n",
    "    finally:\n",
    "        await consumer.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(consume())\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- **AIOKafkaConsumer**: Initializes a Kafka consumer.\n",
    "- **async for msg in consumer**: Asynchronously consumes messages from the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Comparing RabbitMQ and Kafka Integration\n",
    "\n",
    "**Similarities**:\n",
    "\n",
    "- Both use asynchronous clients (`aio-pika`, `aiokafka`).\n",
    "- Both require running a separate service (RabbitMQ or Kafka).\n",
    "- Message publishing and consuming involve similar patterns.\n",
    "\n",
    "**Differences**:\n",
    "\n",
    "- **RabbitMQ**: More suitable for task queues, supports complex routing, and acknowledgments are managed per message.\n",
    "- **Kafka**: Designed for high-throughput, works with streams of data, and consumers track offsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implementing Asynchronous Tasks\n",
    "\n",
    "Let's create a more practical example where FastAPI handles incoming requests and offloads heavy computations to a background worker.\n",
    "\n",
    "### 8.1. Example with RabbitMQ\n",
    "\n",
    "**main.py** (updated)\n",
    "\n",
    "```python\n",
    "@app.post(\"/process-data\")\n",
    "async def process_data(data: dict, background_tasks: BackgroundTasks):\n",
    "    message = json.dumps(data)\n",
    "    background_tasks.add_task(publish_message, message)\n",
    "    return {\"message\": \"Data received and processing started\"}\n",
    "```\n",
    "\n",
    "**worker.py** (updated)\n",
    "\n",
    "```python\n",
    "async def main():\n",
    "    # ... existing code ...\n",
    "\n",
    "                    # Simulate heavy computation\n",
    "                    data = json.loads(message.body.decode())\n",
    "                    result = heavy_computation(data)\n",
    "                    print(f\"Processed data: {result}\")\n",
    "```\n",
    "\n",
    "### 8.2. Example with Kafka\n",
    "\n",
    "**main.py** (updated)\n",
    "\n",
    "```python\n",
    "@app.post(\"/process-data\")\n",
    "async def process_data(data: dict, background_tasks: BackgroundTasks):\n",
    "    message = json.dumps(data)\n",
    "    background_tasks.add_task(send_message, \"task_topic\", message)\n",
    "    return {\"message\": \"Data received and processing started\"}\n",
    "```\n",
    "\n",
    "**consumer.py** (updated)\n",
    "\n",
    "```python\n",
    "async def consume():\n",
    "    # ... existing code ...\n",
    "\n",
    "            data = json.loads(msg.value.decode())\n",
    "            result = heavy_computation(data)\n",
    "            print(f\"Processed data: {result}\")\n",
    "```\n",
    "\n",
    "**Note**: Replace `heavy_computation` with your actual computation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling and Retries\n",
    "\n",
    "### 9.1. RabbitMQ Error Handling\n",
    "\n",
    "- Use **Dead Letter Exchanges** to handle failed messages.\n",
    "- Implement **Retry Mechanisms** using message headers.\n",
    "\n",
    "**worker.py** (handling exceptions)\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # Process message\n",
    "except Exception as e:\n",
    "    print(f\"Error processing message: {e}\")\n",
    "    await message.nack(requeue=False)\n",
    "```\n",
    "\n",
    "### 9.2. Kafka Error Handling\n",
    "\n",
    "- Consumers can commit offsets after processing messages.\n",
    "- Implement **Retry Logic** in the consumer application.\n",
    "\n",
    "**consumer.py** (handling exceptions)\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # Process message\n",
    "except Exception as e:\n",
    "    print(f\"Error processing message: {e}\")\n",
    "    # Decide whether to continue or halt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Testing the Application\n",
    "\n",
    "### 10.1. Testing RabbitMQ Integration\n",
    "\n",
    "Start the FastAPI app:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Run the worker:\n",
    "\n",
    "```bash\n",
    "python worker.py\n",
    "```\n",
    "\n",
    "Send a request:\n",
    "\n",
    "```bash\n",
    "http POST http://localhost:8000/process-data data='{\"value\": 42}'\n",
    "```\n",
    "\n",
    "Check the worker output for processed messages.\n",
    "\n",
    "### 10.2. Testing Kafka Integration\n",
    "\n",
    "Start the FastAPI app:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Run the consumer:\n",
    "\n",
    "```bash\n",
    "python consumer.py\n",
    "```\n",
    "\n",
    "Send a request:\n",
    "\n",
    "```bash\n",
    "http POST http://localhost:8000/process-data data='{\"value\": 42}'\n",
    "```\n",
    "\n",
    "Check the consumer output for processed messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this tutorial, we've:\n",
    "\n",
    "- Understood the roles of RabbitMQ and Apache Kafka as message brokers.\n",
    "- Set up RabbitMQ and Kafka using Docker.\n",
    "- Integrated both RabbitMQ and Kafka with a FastAPI application.\n",
    "- Implemented message publishing and consuming for both systems.\n",
    "- Compared the integration processes and use cases for RabbitMQ and Kafka.\n",
    "- Handled errors and tested the application.\n",
    "\n",
    "By integrating RabbitMQ or Kafka with FastAPI, you can build scalable, asynchronous applications capable of handling high loads and complex workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. References\n",
    "\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [RabbitMQ Official Documentation](https://www.rabbitmq.com/documentation.html)\n",
    "- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [aio-pika Documentation](https://aio-pika.readthedocs.io/en/latest/)\n",
    "- [aiokafka Documentation](https://aiokafka.readthedocs.io/en/stable/)\n",
    "- [Asynchronous Programming in Python](https://docs.python.org/3/library/asyncio.html)\n",
    "- [Message Queue Concepts](https://www.cloudamqp.com/blog/part1-rabbitmq-for-beginners-what-is-rabbitmq.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
